{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト分析\n",
    "\n",
    "- 2018-12-04\n",
    "  -   Kunihiko Saito\n",
    "  -   Yu Ishikawa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 基本ライブラリの読み込み\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 正規表現ライブラリの読み込み\n",
    "import re\n",
    "\n",
    "#　前処理　不必要な記号を削除\n",
    "def preprocessing(document):\n",
    "\n",
    "        lines = document.splitlines()\n",
    "        processed_line = []\n",
    "\n",
    "        horizontal_count = 0\n",
    "\n",
    "        for line in lines:\n",
    "\n",
    "            line =re.sub(r'[!~]', '', line) #半角記号を除去\n",
    "            line =re.sub(r'[︰＠]', '', line) #全角記号を除去\n",
    "            line = re.sub('\\ufeff', '', line) # \\ufeffを除去\n",
    "\n",
    "            processed_line.append(line)\n",
    "\n",
    "        return ''.join(processed_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x93 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-70626be191c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mrow_documents\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x93 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "#　ファイルの読み込み\n",
    "\n",
    "code='shift-jis'\n",
    "code='utf-8'#標準コード\n",
    "file='./toshiba.txt'\n",
    "\n",
    "with open(file, 'r', encoding=code) as f:\n",
    "    row_documents= f.read()\n",
    "\n",
    "docs=preprocessing(row_documents)\n",
    "docs[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分かち書き分析\n",
    "名詞をカウント時\n",
    "\n",
    "- 名詞で\n",
    "features[1] == '一般'\n",
    "とすると「電話」など、動詞としても使える名詞がカウントされない\n",
    "\n",
    "- ーのコードが２種類存在　読めないものがある\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(row_documents.splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! Mecab 'Hello World K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natto import MeCab\n",
    "def morphological(document):\n",
    "\n",
    "        word_list = []\n",
    "        #MeCabの形態素解析結果のフォーマット\n",
    "        with MeCab('-F%f[0],%f[1],%f[6]') as mcb:\n",
    "            for i, token in enumerate (mcb.parse(document, as_nodes=True)):\n",
    "                features = token.feature.split(',')\n",
    "                #名詞（一般）動詞（自立）、形容詞（自立）以外は除外\n",
    "                if features[0] == '名詞' and features[1] == '一般' and features[2] != '':\n",
    "                #if features[0] == '名詞'  and features[2] != '':\n",
    "                    word_list.append(features[2])\n",
    "                if features[0] == '動詞' and features[2] != '':\n",
    "                    #word_list.append(features[2])  #動詞は無視\n",
    "                    pass\n",
    "                if features[0] == '形容詞' and features[1] == '自立' and features[2] != '':\n",
    "                    word_list.append(features[2])\n",
    "                \n",
    "                if i%10000==0:\n",
    "                    print(i, end=\" \")\n",
    "        return word_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分かち書き一覧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nattoがうまく動作しない場合\n",
    "\n",
    "%env\n",
    "で以下を確認\n",
    "- 'PATH'に　C:\\\\Program Files\\\\MeCab\\\\bin　がある\n",
    "- 'MECARC'が存在する\n",
    "- 'MECAB_PATH'が存在する\n",
    "\n",
    "存在しない場合は以下のコマンドで作成\n",
    "\n",
    "os.environ['MECABRC']='C:\\\\Program Files\\\\MeCab\\\\etc\\\\mecabrc'\n",
    "\n",
    "os.environ['MECAB_PATH']='C:\\\\Program Files\\\\MeCab\\\\bin\\\\libmecab.dll'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict=morphological(docs)\n",
    "dict=morphological(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 単語の数え上げ\n",
    "\n",
    " itertoolsでさらっとカウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "# カウント処理のためのライブラリ\n",
    "from collections import Counter\n",
    "word_freq = Counter(itertools.chain(dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#頻度の多いものを出力\n",
    "word_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ソート\n",
    "\n",
    "- dec_sort 降順にソート\n",
    "- up_sort 昇順ソート\n",
    "\n",
    "\n",
    ".most_commonを使えばソートの必要はない？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_sort=sorted(word_freq.items(), key=lambda x: -x[1])\n",
    "dec_sort[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_sort=sorted(word_freq.items(), key=lambda x: x[1])\n",
    "up_sort[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語の頻度降順に並べ替え\n",
    "dic = []\n",
    "for word_uniq in word_freq.most_common():\n",
    "    dic.append(word_uniq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 並べ替えた単語を表示\n",
    "dic[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語にIDを付与し、辞書の完成\n",
    "dic_inv = {}\n",
    "for i, word_uniq in enumerate(dic, start=1):\n",
    "    dic_inv.update({word_uniq: i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辞書の中身を表示\n",
    "dic_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辞書のサイズを表示\n",
    "len(dic_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 入力ファイルのからテキストを1行づつ取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = row_documents.splitlines()\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テキストをコード化する\n",
    "- lines[0]:最初のテキスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list=[]\n",
    "with MeCab('-F%f[6]') as mcb:\n",
    "    for token in mcb.parse(lines[0], as_nodes=True):        \n",
    "        features = token.feature.split(',')\n",
    "        key=features\n",
    "        if(key[0] in dic_inv):\n",
    "            list.append(dic_inv[key[0]])\n",
    "\n",
    "print(list)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全テキストをコード化\n",
    "ディープラーニングの入力を作成\n",
    "\n",
    "時間がかかる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists=[]\n",
    "\n",
    "with MeCab('-F%f[6]') as mcb:\n",
    "    for line in lines:\n",
    "        list=[]    \n",
    "        for token in mcb.parse(line, as_nodes=True):        \n",
    "            features = token.feature.split(',')\n",
    "            key=features\n",
    "            if(key[0] in dic_inv):\n",
    "                list.append(dic_inv[key[0]])\n",
    "    lists.append(list)\n",
    "    #print(list)\n",
    "\n",
    "len(lists)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "nplist = np.array(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(nplist[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 再度、分かち書きを行い確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-54d8e206edbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"-Ochasen\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lines' is not defined"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "m = MeCab.Tagger (\"-Ochasen\")\n",
    "print(m.parse (lines[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下ディープラーニングLSTM　正解データが必要\n",
    "\n",
    "- 故障と関係あるテキストに１を紐づける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ライブラリの読み込み ##\n",
    "\n",
    "# TensorFlowライブラリ\n",
    "import tensorflow as tf\n",
    "# TFLearnライブラリ\n",
    "import tflearn\n",
    "# データの前処理を行うライブラリ\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストの単語をIDへ変換し配列へ格納\n",
    "# 単語データの配列のサイズを合わせる\n",
    "trainX = pad_sequences(lists, maxlen=50, value=0.)\n",
    "\n",
    "# 正解データを配列へ格納\n",
    "# 正解データのサイズを合わせる\n",
    "#trainY =????\n",
    "#trainY = to_categorical(trainY, nb_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ニューラルネットワークの作成 ##\n",
    "\n",
    "\n",
    "input_dim=75826\n",
    "\n",
    "## 初期化\n",
    "tf.reset_default_graph()\n",
    "\n",
    "## 入力層の作成\n",
    "net = tflearn.input_data([None, 32])\n",
    "\n",
    "## 中間層の作成\n",
    "# 単語埋め込み層\n",
    "net = tflearn.embedding(net, input_dim=75826, output_dim=128)\n",
    "\n",
    "# LSTMブロック\n",
    "net = tflearn.lstm(net, 128, dropout=0.5)\n",
    "\n",
    "## 出力層の作成 \n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 4．モデルの作成（学習） ##\n",
    "# 学習の実行\n",
    "model = tflearn.DNN(net)\n",
    "model.fit(trainX, trainY, n_epoch=50, batch_size=32, validation_set=0.2, shuffle=True, show_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflearn.datasets import imdb\n",
    "train, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000,valid_portion=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
